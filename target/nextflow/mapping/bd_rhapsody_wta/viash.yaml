functionality:
  name: "bd_rhapsody_wta"
  namespace: "mapping"
  version: "main_build"
  authors:
  - name: "Robrecht Cannoodt"
    email: "rcannood@gmail.com"
    roles:
    - "maintainer"
    props:
      github: "rcannood"
      orcid: "0000-0003-3641-729X"
  inputs: []
  outputs: []
  arguments:
  - type: "file"
    name: "--input"
    alternatives:
    - "-i"
    description: "Path to your read files in the FASTQ.GZ format. You may specify\
      \ as many R1/R2 read pairs as you want."
    example:
    - "input.fastq.gz"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "file"
    name: "--reference_genome"
    alternatives:
    - "-r"
    description: "Path to STAR index as a tar.gz file."
    example:
    - "reference_genome.tar.gz"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--transcriptome_annotation"
    alternatives:
    - "-t"
    description: "Path to GTF annotation file."
    example:
    - "transcriptome.gtf"
    default: []
    must_exist: false
    required: true
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--abseq_reference"
    alternatives:
    - "-a"
    description: "Path to the AbSeq reference file in FASTA format. Only needed if\
      \ BD AbSeq Ab-Oligos are used."
    example: []
    default: []
    must_exist: false
    required: false
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "file"
    name: "--supplemental_reference"
    alternatives:
    - "-s"
    description: "Path to the supplemental reference file in FASTA format. Only needed\
      \ if there are additional transgene sequences used in the experiment."
    example: []
    default: []
    must_exist: false
    required: false
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "file"
    name: "--output"
    alternatives:
    - "-o"
    description: "Output folder. Output still needs to be processed further."
    example:
    - "output_dir"
    default: []
    must_exist: false
    required: true
    direction: "output"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--exact_cell_count"
    alternatives: []
    description: "Exact cell count - Set a specific number (>=1) of cells as putative,\
      \ based on those with the highest error-corrected read count"
    example:
    - 10000
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "boolean_true"
    name: "--disable_putative_calling"
    alternatives: []
    description: "Disable Refined Putative Cell Calling - Determine putative cells\
      \ using only the basic algorithm (minimum second derivative along the cumulative\
      \ reads curve). The refined algorithm attempts to remove false positives and\
      \ recover false negatives, but may not be ideal for certain complex mixtures\
      \ of cell types. Does not apply if Exact Cell Count is set."
    direction: "input"
  - type: "double"
    name: "--subsample"
    alternatives: []
    description: "A number >1 or fraction (0 < n < 1) to indicate the number or percentage\
      \ of reads to subsample."
    example:
    - 0.01
    default: []
    required: false
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--subsample_seed"
    alternatives: []
    description: "A seed for replicating a previous subsampled run."
    example:
    - 3445
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--sample_tags_version"
    alternatives: []
    description: "Specify if multiplexed run."
    example:
    - "human"
    default: []
    required: false
    choices:
    - "human"
    - "hs"
    - "mouse"
    - "mm"
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "string"
    name: "--tag_names"
    alternatives: []
    description: "Tag_Names (optional) - Specify the tag number followed by '-' and\
      \ the desired sample name to appear in Sample_Tag_Metrics.csv\nDo not use the\
      \ special characters: &, (), [], {},  <>, ?, |\n"
    example:
    - "4-mySample"
    - "9-myOtherSample"
    - "6-alsoThisSample"
    default: []
    required: false
    choices: []
    direction: "input"
    multiple: true
    multiple_sep: ":"
  - type: "boolean"
    name: "--parallel"
    alternatives: []
    description: "Run jobs in parallel."
    example: []
    default:
    - true
    required: false
    direction: "input"
    multiple: false
    multiple_sep: ":"
  - type: "boolean_true"
    name: "--timestamps"
    alternatives: []
    description: "Add timestamps to the errors, warnings, and notifications."
    direction: "input"
  resources:
  - type: "file"
    text: |
      // bd_rhapsody_wta main_build
      // 
      // This wrapper script is auto-generated by viash 0.5.13 and is thus a derivative
      // work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
      // Intuitive.
      // 
      // The component may contain files which fall under a different license. The
      // authors of this component should specify the license in the header of such
      // files, or include a separate license file detailing the licenses of all included
      // files.
      // 
      // Component authors:
      //  * Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid:
      // 0000-0003-3641-729X}
      
      nextflow.enable.dsl=2
      
      // Required imports
      import groovy.json.JsonSlurper
      
      // initialise slurper
      def jsonSlurper = new JsonSlurper()
      
      // DEFINE CUSTOM CODE
      
      // functionality metadata
      thisFunctionality = [
        'name': 'bd_rhapsody_wta',
        'arguments': [
          [
            'name': 'input',
            'required': true,
            'type': 'file',
            'direction': 'input',
            'description': 'Path to your read files in the FASTQ.GZ format. You may specify as many R1/R2 read pairs as you want.',
            'example': ['input.fastq.gz'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'reference_genome',
            'required': true,
            'type': 'file',
            'direction': 'input',
            'description': 'Path to STAR index as a tar.gz file.',
            'example': 'reference_genome.tar.gz',
            'multiple': false
          ],
          [
            'name': 'transcriptome_annotation',
            'required': true,
            'type': 'file',
            'direction': 'input',
            'description': 'Path to GTF annotation file.',
            'example': 'transcriptome.gtf',
            'multiple': false
          ],
          [
            'name': 'abseq_reference',
            'required': false,
            'type': 'file',
            'direction': 'input',
            'description': 'Path to the AbSeq reference file in FASTA format. Only needed if BD AbSeq Ab-Oligos are used.',
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'supplemental_reference',
            'required': false,
            'type': 'file',
            'direction': 'input',
            'description': 'Path to the supplemental reference file in FASTA format. Only needed if there are additional transgene sequences used in the experiment.',
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'output',
            'required': true,
            'type': 'file',
            'direction': 'output',
            'description': 'Output folder. Output still needs to be processed further.',
            'default': '$id.$key.output',
            'example': 'output_dir',
            'multiple': false
          ],
          [
            'name': 'exact_cell_count',
            'required': false,
            'type': 'integer',
            'direction': 'input',
            'description': 'Exact cell count - Set a specific number (>=1) of cells as putative, based on those with the highest error-corrected read count',
            'example': 10000,
            'multiple': false
          ],
          [
            'name': 'disable_putative_calling',
            'required': false,
            'type': 'boolean_true',
            'direction': 'input',
            'description': 'Disable Refined Putative Cell Calling - Determine putative cells using only the basic algorithm (minimum second derivative along the cumulative reads curve). The refined algorithm attempts to remove false positives and recover false negatives, but may not be ideal for certain complex mixtures of cell types. Does not apply if Exact Cell Count is set.',
            'default': false,
            'multiple': false
          ],
          [
            'name': 'subsample',
            'required': false,
            'type': 'double',
            'direction': 'input',
            'description': 'A number >1 or fraction (0 < n < 1) to indicate the number or percentage of reads to subsample.',
            'example': 0.01,
            'multiple': false
          ],
          [
            'name': 'subsample_seed',
            'required': false,
            'type': 'integer',
            'direction': 'input',
            'description': 'A seed for replicating a previous subsampled run.',
            'example': 3445,
            'multiple': false
          ],
          [
            'name': 'sample_tags_version',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Specify if multiplexed run.',
            'example': 'human',
            'multiple': false
          ],
          [
            'name': 'tag_names',
            'required': false,
            'type': 'string',
            'direction': 'input',
            'description': 'Tag_Names (optional) - Specify the tag number followed by \'-\' and the desired sample name to appear in Sample_Tag_Metrics.csv\nDo not use the special characters: &, (), [], {},  <>, ?, |\n',
            'example': ['4-mySample\', \'9-myOtherSample\', \'6-alsoThisSample'],
            'multiple': true,
            'multiple_sep': ':'
          ],
          [
            'name': 'parallel',
            'required': false,
            'type': 'boolean',
            'direction': 'input',
            'description': 'Run jobs in parallel.',
            'default': true,
            'multiple': false
          ],
          [
            'name': 'timestamps',
            'required': false,
            'type': 'boolean_true',
            'direction': 'input',
            'description': 'Add timestamps to the errors, warnings, and notifications.',
            'default': false,
            'multiple': false
          ]
        ]
      ]
      
      thisHelpMessage = '''bd_rhapsody_wta main_build
      
      A wrapper for the BD Rhapsody Analysis CWL v1.9.1 pipeline.
      
      The CWL pipeline file is obtained by cloning
      'https://bitbucket.org/CRSwDev/cwl/src/master/' and removing all objects with
      class 'DockerRequirement' from the YML.
      
      The reference_genome and transcriptome_annotation files can be downloaded from
      these locations:
        - Human:
      http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCh38-PhiX-gencodev29/
        - Mouse:
      http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCm38-PhiX-gencodevM19/
      
      Options:
          --input
              type: file, required parameter, multiple values allowed
              example: input.fastq.gz
              Path to your read files in the FASTQ.GZ format. You may specify as many
      R1/R2 read pairs as you want.
      
          --reference_genome
              type: file, required parameter
              example: reference_genome.tar.gz
              Path to STAR index as a tar.gz file.
      
          --transcriptome_annotation
              type: file, required parameter
              example: transcriptome.gtf
              Path to GTF annotation file.
      
          --abseq_reference
              type: file, multiple values allowed
              Path to the AbSeq reference file in FASTA format. Only needed if BD
      AbSeq Ab-Oligos are used.
      
          --supplemental_reference
              type: file, multiple values allowed
              Path to the supplemental reference file in FASTA format. Only needed if
      there are additional transgene sequences used in the experiment.
      
          --output
              type: file, required parameter, output
              example: output_dir
              Output folder. Output still needs to be processed further.
      
          --exact_cell_count
              type: integer
              example: 10000
              Exact cell count - Set a specific number (>=1) of cells as putative,
      based on those with the highest error-corrected read count
      
          --disable_putative_calling
              type: boolean_true
              Disable Refined Putative Cell Calling - Determine putative cells using
      only the basic algorithm (minimum second derivative along the cumulative reads
      curve). The refined algorithm attempts to remove false positives and recover
      false negatives, but may not be ideal for certain complex mixtures of cell
      types. Does not apply if Exact Cell Count is set.
      
          --subsample
              type: double
              example: 0.01
              A number >1 or fraction (0 < n < 1) to indicate the number or percentage
      of reads to subsample.
      
          --subsample_seed
              type: integer
              example: 3445
              A seed for replicating a previous subsampled run.
      
          --sample_tags_version
              type: string
              example: human
              choices: [ human, hs, mouse, mm ]
              Specify if multiplexed run.
      
          --tag_names
              type: string, multiple values allowed
              example: 4-mySample:9-myOtherSample:6-alsoThisSample
              Tag_Names (optional) - Specify the tag number followed by '-' and the
      desired sample name to appear in Sample_Tag_Metrics.csv
              Do not use the special characters: &, (), [], {},  <>, ?, |
      
          --parallel
              type: boolean
              default: true
              Run jobs in parallel.
      
          --timestamps
              type: boolean_true
              Add timestamps to the errors, warnings, and notifications.'''
      
      thisScript = '''set -e
      tempscript=".viash_script.sh"
      cat > "$tempscript" << VIASHMAIN
      import os
      import re
      import subprocess
      import tempfile
      
      ## VIASH START
      # The following code has been auto-generated by Viash.
      par = {
        'input': $( if [ ! -z ${VIASH_PAR_INPUT+x} ]; then echo "'${VIASH_PAR_INPUT//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'reference_genome': $( if [ ! -z ${VIASH_PAR_REFERENCE_GENOME+x} ]; then echo "'${VIASH_PAR_REFERENCE_GENOME//\\'/\\\\\\'}'"; else echo None; fi ),
        'transcriptome_annotation': $( if [ ! -z ${VIASH_PAR_TRANSCRIPTOME_ANNOTATION+x} ]; then echo "'${VIASH_PAR_TRANSCRIPTOME_ANNOTATION//\\'/\\\\\\'}'"; else echo None; fi ),
        'abseq_reference': $( if [ ! -z ${VIASH_PAR_ABSEQ_REFERENCE+x} ]; then echo "'${VIASH_PAR_ABSEQ_REFERENCE//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'supplemental_reference': $( if [ ! -z ${VIASH_PAR_SUPPLEMENTAL_REFERENCE+x} ]; then echo "'${VIASH_PAR_SUPPLEMENTAL_REFERENCE//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'output': $( if [ ! -z ${VIASH_PAR_OUTPUT+x} ]; then echo "'${VIASH_PAR_OUTPUT//\\'/\\\\\\'}'"; else echo None; fi ),
        'exact_cell_count': $( if [ ! -z ${VIASH_PAR_EXACT_CELL_COUNT+x} ]; then echo "int('${VIASH_PAR_EXACT_CELL_COUNT//\\'/\\\\\\'}')"; else echo None; fi ),
        'disable_putative_calling': $( if [ ! -z ${VIASH_PAR_DISABLE_PUTATIVE_CALLING+x} ]; then echo "'${VIASH_PAR_DISABLE_PUTATIVE_CALLING//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi ),
        'subsample': $( if [ ! -z ${VIASH_PAR_SUBSAMPLE+x} ]; then echo "float('${VIASH_PAR_SUBSAMPLE//\\'/\\\\\\'}')"; else echo None; fi ),
        'subsample_seed': $( if [ ! -z ${VIASH_PAR_SUBSAMPLE_SEED+x} ]; then echo "int('${VIASH_PAR_SUBSAMPLE_SEED//\\'/\\\\\\'}')"; else echo None; fi ),
        'sample_tags_version': $( if [ ! -z ${VIASH_PAR_SAMPLE_TAGS_VERSION+x} ]; then echo "'${VIASH_PAR_SAMPLE_TAGS_VERSION//\\'/\\\\\\'}'"; else echo None; fi ),
        'tag_names': $( if [ ! -z ${VIASH_PAR_TAG_NAMES+x} ]; then echo "'${VIASH_PAR_TAG_NAMES//\\'/\\\\\\'}'.split(':')"; else echo None; fi ),
        'parallel': $( if [ ! -z ${VIASH_PAR_PARALLEL+x} ]; then echo "'${VIASH_PAR_PARALLEL//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi ),
        'timestamps': $( if [ ! -z ${VIASH_PAR_TIMESTAMPS+x} ]; then echo "'${VIASH_PAR_TIMESTAMPS//\\'/\\\\\\'}'.lower() == 'true'"; else echo None; fi )
      }
      meta = {
        'functionality_name': '$VIASH_META_FUNCTIONALITY_NAME',
        'resources_dir': '$VIASH_META_RESOURCES_DIR',
        'temp_dir': '$VIASH_TEMP'
      }
      
      resources_dir = '$VIASH_META_RESOURCES_DIR'
      
      ## VIASH END
      
      def strip_margin(text):
        return re.sub('\\\\n[ \\\\t]*\\\\|', '\\\\n', text)
      
      # if par_input is a directory, look for fastq files
      if len(par["input"]) == 1 and os.path.isdir(par["input"][0]):
        par["input"] = [ os.path.join(dp, f) for dp, dn, filenames in os.walk(par["input"]) for f in filenames if re.match(r'.*\\\\.fastq.gz', f) ]
      
      # use absolute paths
      par["input"] = [ os.path.abspath(f) for f in par["input"] ]
      par["reference_genome"] = os.path.abspath(par["reference_genome"])
      par["transcriptome_annotation"] = os.path.abspath(par["transcriptome_annotation"])
      par["output"] = os.path.abspath(par["output"])
      if par["abseq_reference"]:
        par["abseq_reference"] = [ os.path.abspath(f) for f in par["abseq_reference"] ]
      if par["supplemental_reference"]:
        par["supplemental_reference"] = [ os.path.abspath(f) for f in par["supplemental_reference"] ]
      
      # create output dir if not exists
      if not os.path.exists(par["output"]):
        os.makedirs(par["output"])
      
      # Create params file
      config_file = os.path.join(par["output"], "config.yml")
      endl = "\\\\n"
      
      content_list = [f"""#!/usr/bin/env cwl-runner
      
      cwl:tool: rhapsody
      
      # This is a YML file used to specify the inputs for a BD Genomics WTA Rhapsody Analysis pipeline run. See the
      # BD Genomics Analysis Setup User Guide (Doc ID: 47383) for more details.
      
      ## Reads (required) - Path to your read files in the FASTQ.GZ format. You may specify as many R1/R2 read pairs as you want.
      Reads:
      """]
      
      for file in par["input"]:
        content_list.append(strip_margin(f"""\\\\
       - class: File
         location: "{file}"
      """))
      
      content_list.append(strip_margin(f"""\\\\
      
      ## Reference_Genome (required) - Path to STAR index for tar.gz format. See Doc ID: 47383 for instructions to obtain pre-built STAR index file.
      Reference_Genome:
         class: File
         location: "{par["reference_genome"]}"
      
      ## Transcriptome_Annotation (required) - Path to GTF annotation file
      Transcriptome_Annotation:
         class: File
         location: "{par["transcriptome_annotation"]}"
      """))
      
      if par["abseq_reference"]:
        content_list.append(strip_margin(f"""\\\\
      
      ## AbSeq_Reference (optional) - Path to the AbSeq reference file in FASTA format.  Only needed if BD AbSeq Ab-Oligos are used.
      ## For putative cell calling using an AbSeq dataset, please provide an AbSeq reference fasta file as the AbSeq_Reference.
      AbSeq_Reference:
      """))
        for file in par["abseq_reference"]:
          content_list.append(strip_margin(f"""\\\\
       - class: File
         location: {file}
      """))
      
      if par["supplemental_reference"]:
        content_list.append(strip_margin(f"""\\\\
      
      ## Supplemental_Reference (optional) - Path to the supplemental reference file in FASTA format.  Only needed if there are additional transgene sequences used in the experiment.
      Supplemental_Reference:
      """))
        for file in par["supplemental_reference"]:
          content_list.append(strip_margin(f"""\\\\
       - class: File
         location: {file}
      """))
      
      ## Putative Cell Calling Settings
      content_list.append(strip_margin(f"""\\\\
      
      ####################################
      ## Putative Cell Calling Settings ##
      ####################################
      """))
      
      if par["exact_cell_count"]:
        content_list.append(strip_margin(f"""\\\\
      ## Exact cell count (optional) - Set a specific number (>=1) of cells as putative, based on those with the highest error-corrected read count
      Exact_Cell_Count: {par["exact_cell_count"]}
      """))
      
      if par["disable_putative_calling"]:
        content_list.append(strip_margin(f"""\\\\
      ## Disable Refined Putative Cell Calling (optional) - Determine putative cells using only the basic algorithm (minimum second derivative along the cumulative reads curve).  The refined algorithm attempts to remove false positives and recover false negatives, but may not be ideal for certain complex mixtures of cell types.  Does not apply if Exact Cell Count is set.
      ## The values can be true or false. By default, the refined algorithm is used.
      Basic_Algo_Only: {str(par["disable_putative_calling"]).lower()}
      """))
      
      ## Subsample Settings
      content_list.append(strip_margin(f"""\\\\
      
      ########################
      ## Subsample Settings ##
      ########################
      """
      ))
      
      if par["subsample"]:
        content_list.append(strip_margin(f"""\\\\
      ## Subsample (optional) - A number >1 or fraction (0 < n < 1) to indicate the number or percentage of reads to subsample.
      Subsample: {par["subsample"]}
      """))
      
      if par["subsample_seed"]:
        content_list.append(strip_margin(f"""\\\\
      ## Subsample seed (optional) - A seed for replicating a previous subsampled run.
      Subsample_seed: {par["subsample_seed"]}
      """))
      
      
      ## Multiplex options
      content_list.append(strip_margin(f"""\\\\
      
      #######################
      ## Multiplex options ##
      #######################
      """
      ))
      
      if par["sample_tags_version"]:
        content_list.append(strip_margin(f"""\\\\
      ## Sample Tags Version (optional) - Specify if multiplexed run: human, hs, mouse or mm
      Sample_Tags_Version: {par["sample_tags_version"]}
      """))
      
      if par["tag_names"]:
        content_list.append(strip_margin(f"""\\\\
      ## Tag_Names (optional) - Specify the tag number followed by '-' and the desired sample name to appear in Sample_Tag_Metrics.csv
      # Do not use the special characters: &, (), [], {{}},  <>, ?, |
      Tag_Names: [{', '.join(par["tag_names"])}]
      """))
      
      ## Write config to file
      config_content = ''.join(content_list)
      
      with open(config_file, "w") as f:
        f.write(config_content)
      
      ## Process parameters
      proc_pars = ["--no-container"]
      # proc_pars = []
      
      if par["parallel"]:
        proc_pars.append("--parallel")
      
      if par["timestamps"]:
        proc_pars.append("--timestamps")
      
      ## Run pipeline
      cwl_file=os.path.abspath(os.path.join(meta["resources_dir"], "rhapsody_wta_1.9.1_nodocker.cwl"))
      
      with tempfile.TemporaryDirectory(prefix="cwl-bd_rhapsody_wta-", dir=meta["temp_dir"]) as temp_dir:
        cmd = ["cwl-runner"] + proc_pars + [cwl_file, os.path.basename(config_file)]
      
        env = dict(os.environ)
        env["TMPDIR"] = temp_dir
      
        print("> " + ' '.join(cmd))
      
        p = subprocess.Popen(
          cmd,
          cwd=os.path.dirname(config_file),
          env=env
        )
        p.wait()
      
        if p.returncode != 0:
          raise Exception(f"cwl-runner finished with exit code {p.returncode}") 
      VIASHMAIN
      python "$tempscript"
      '''
      
      thisDefaultProcessArgs = [
        // key to be used to trace the process and determine output names
        key: thisFunctionality.name,
        // fixed arguments to be passed to script
        args: [:],
        // default directives
        directives: jsonSlurper.parseText("""{
        "container" : {
          "registry" : "ghcr.io",
          "image" : "openpipelines-bio/mapping_bd_rhapsody_wta",
          "tag" : "main_build"
        },
        "label" : [
          "highmem",
          "highcpu"
        ]
      }"""),
        // auto settings
        auto: jsonSlurper.parseText("""{
        "simplifyInput" : true,
        "simplifyOutput" : true,
        "transcript" : false,
        "publish" : false
      }"""),
        // apply a map over the incoming tuple
        // example: { tup -> [ tup[0], [input: tup[1].output], tup[2] ] }
        map: null,
        // apply a map over the ID element of a tuple (i.e. the first element)
        // example: { id -> id + "_foo" }
        mapId: null,
        // apply a map over the data element of a tuple (i.e. the second element)
        // example: { data -> [ input: data.output ] }
        mapData: null,
        // apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements)
        // example: { pt -> pt.drop(1) }
        mapPassthrough: null,
        // rename keys in the data field of the tuple (i.e. the second element)
        // example: [ "new_key": "old_key" ]
        renameKeys: null,
        // whether or not to print debug messages
        debug: false
      ]
      
      // END CUSTOM CODE
      
      import nextflow.Nextflow
      import nextflow.script.IncludeDef
      import nextflow.script.ScriptBinding
      import nextflow.script.ScriptMeta
      import nextflow.script.ScriptParser
      
      // retrieve resourcesDir here to make sure the correct path is found
      resourcesDir = ScriptMeta.current().getScriptPath().getParent()
      
      def assertMapKeys(map, expectedKeys, requiredKeys, mapName) {
        assert map instanceof Map : "Expected argument '$mapName' to be a Map. Found: class ${map.getClass()}"
        map.forEach { key, val -> 
          assert key in expectedKeys : "Unexpected key '$key' in ${mapName ? mapName + " " : ""}map"
        }
        requiredKeys.forEach { requiredKey -> 
          assert map.containsKey(requiredKey) : "Missing required key '$key' in ${mapName ? mapName + " " : ""}map"
        }
      }
      
      // TODO: unit test processDirectives
      def processDirectives(Map drctv) {
        // remove null values
        drctv = drctv.findAll{k, v -> v != null}
      
        /* DIRECTIVE accelerator
          accepted examples:
          - [ limit: 4, type: "nvidia-tesla-k80" ]
        */
        if (drctv.containsKey("accelerator")) {
          assertMapKeys(drctv["accelerator"], ["type", "limit", "request", "runtime"], [], "accelerator")
        }
      
        /* DIRECTIVE afterScript
          accepted examples:
          - "source /cluster/bin/cleanup"
        */
        if (drctv.containsKey("afterScript")) {
          assert drctv["afterScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE beforeScript
          accepted examples:
          - "source /cluster/bin/setup"
        */
        if (drctv.containsKey("beforeScript")) {
          assert drctv["beforeScript"] instanceof CharSequence
        }
      
        /* DIRECTIVE cache
          accepted examples:
          - true
          - false
          - "deep"
          - "lenient"
        */
        if (drctv.containsKey("cache")) {
          assert drctv["cache"] instanceof CharSequence || drctv["cache"] instanceof Boolean
          if (drctv["cache"] instanceof CharSequence) {
            assert drctv["cache"] in ["deep", "lenient"] : "Unexpected value for cache"
          }
        }
      
        /* DIRECTIVE conda
          accepted examples:
          - "bwa=0.7.15"
          - "bwa=0.7.15 fastqc=0.11.5"
          - ["bwa=0.7.15", "fastqc=0.11.5"]
        */
        if (drctv.containsKey("conda")) {
          if (drctv["conda"] instanceof List) {
            drctv["conda"] = drctv["conda"].join(" ")
          }
          assert drctv["conda"] instanceof CharSequence
        }
      
        /* DIRECTIVE container
          accepted examples:
          - "foo/bar:tag"
          - [ registry: "reg", image: "im", tag: "ta" ]
            is transformed to "reg/im:ta"
          - [ image: "im" ] 
            is transformed to "im:latest"
        */
        if (drctv.containsKey("container")) {
          assert drctv["container"] instanceof Map || drctv["container"] instanceof CharSequence
          if (drctv["container"] instanceof Map) {
            def m = drctv["container"]
            assertMapKeys(m, [ "registry", "image", "tag" ], ["image"], "container")
            def part1 = 
              params.containsKey("override_container_registry") ? params["override_container_registry"] + "/" : 
              m.registry ? m.registry + "/" : 
              ""
            def part2 = m.image
            def part3 = m.tag ? ":" + m.tag : ":latest"
            drctv["container"] = part1 + part2 + part3
          }
        }
      
        /* DIRECTIVE containerOptions
          accepted examples:
          - "--foo bar"
          - ["--foo bar", "-f b"]
        */
        if (drctv.containsKey("containerOptions")) {
          if (drctv["containerOptions"] instanceof List) {
            drctv["containerOptions"] = drctv["containerOptions"].join(" ")
          }
          assert drctv["containerOptions"] instanceof CharSequence
        }
      
        /* DIRECTIVE cpus
          accepted examples:
          - 1
          - 10
        */
        if (drctv.containsKey("cpus")) {
          assert drctv["cpus"] instanceof Integer
        }
      
        /* DIRECTIVE disk
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("disk")) {
          assert drctv["disk"] instanceof CharSequence
          // assert drctv["disk"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE echo
          accepted examples:
          - true
          - false
        */
        if (drctv.containsKey("echo")) {
          assert drctv["echo"] instanceof Boolean
        }
      
        /* DIRECTIVE errorStrategy
          accepted examples:
          - "terminate"
          - "finish"
        */
        if (drctv.containsKey("errorStrategy")) {
          assert drctv["errorStrategy"] instanceof CharSequence
          assert drctv["errorStrategy"] in ["terminate", "finish", "ignore", "retry"] : "Unexpected value for errorStrategy"
        }
      
        /* DIRECTIVE executor
          accepted examples:
          - "local"
          - "sge"
        */
        if (drctv.containsKey("executor")) {
          assert drctv["executor"] instanceof CharSequence
          assert drctv["executor"] in ["local", "sge", "uge", "lsf", "slurm", "pbs", "pbspro", "moab", "condor", "nqsii", "ignite", "k8s", "awsbatch", "google-pipelines"] : "Unexpected value for executor"
        }
      
        /* DIRECTIVE machineType
          accepted examples:
          - "n1-highmem-8"
        */
        if (drctv.containsKey("machineType")) {
          assert drctv["machineType"] instanceof CharSequence
        }
      
        /* DIRECTIVE maxErrors
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxErrors")) {
          assert drctv["maxErrors"] instanceof Integer
        }
      
        /* DIRECTIVE maxForks
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxForks")) {
          assert drctv["maxForks"] instanceof Integer
        }
      
        /* DIRECTIVE maxRetries
          accepted examples:
          - 1
          - 3
        */
        if (drctv.containsKey("maxRetries")) {
          assert drctv["maxRetries"] instanceof Integer
        }
      
        /* DIRECTIVE memory
          accepted examples:
          - "1 GB"
          - "2TB"
          - "3.2KB"
          - "10.B"
        */
        if (drctv.containsKey("memory")) {
          assert drctv["memory"] instanceof CharSequence
          // assert drctv["memory"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
          // ^ does not allow closures
        }
      
        /* DIRECTIVE module
          accepted examples:
          - "ncbi-blast/2.2.27"
          - "ncbi-blast/2.2.27:t_coffee/10.0"
          - ["ncbi-blast/2.2.27", "t_coffee/10.0"]
        */
        if (drctv.containsKey("module")) {
          if (drctv["module"] instanceof List) {
            drctv["module"] = drctv["module"].join(":")
          }
          assert drctv["module"] instanceof CharSequence
        }
      
        /* DIRECTIVE penv
          accepted examples:
          - "smp"
        */
        if (drctv.containsKey("penv")) {
          assert drctv["penv"] instanceof CharSequence
        }
      
        /* DIRECTIVE pod
          accepted examples:
          - [ label: "key", value: "val" ]
          - [ annotation: "key", value: "val" ]
          - [ env: "key", value: "val" ]
          - [ [label: "l", value: "v"], [env: "e", value: "v"]]
        */
        if (drctv.containsKey("pod")) {
          if (drctv["pod"] instanceof Map) {
            drctv["pod"] = [ drctv["pod"] ]
          }
          assert drctv["pod"] instanceof List
          drctv["pod"].forEach { pod ->
            assert pod instanceof Map
            // TODO: should more checks be added?
            // See https://www.nextflow.io/docs/latest/process.html?highlight=directives#pod
            // e.g. does it contain 'label' and 'value', or 'annotation' and 'value', or ...?
          }
        }
      
        /* DIRECTIVE publishDir
          accepted examples:
          - []
          - [ [ path: "foo", enabled: true ], [ path: "bar", enabled: false ] ]
          - "/path/to/dir" 
            is transformed to [[ path: "/path/to/dir" ]]
          - [ path: "/path/to/dir", mode: "cache" ]
            is transformed to [[ path: "/path/to/dir", mode: "cache" ]]
        */
        // TODO: should we also look at params["publishDir"]?
        if (drctv.containsKey("publishDir")) {
          def pblsh = drctv["publishDir"]
          
          // check different options
          assert pblsh instanceof List || pblsh instanceof Map || pblsh instanceof CharSequence
          
          // turn into list if not already so
          // for some reason, 'if (!pblsh instanceof List) pblsh = [ pblsh ]' doesn't work.
          pblsh = pblsh instanceof List ? pblsh : [ pblsh ]
      
          // check elements of publishDir
          pblsh = pblsh.collect{ elem ->
            // turn into map if not already so
            elem = elem instanceof CharSequence ? [ path: elem ] : elem
      
            // check types and keys
            assert elem instanceof Map : "Expected publish argument '$elem' to be a String or a Map. Found: class ${elem.getClass()}"
            assertMapKeys(elem, [ "path", "mode", "overwrite", "pattern", "saveAs", "enabled" ], ["path"], "publishDir")
      
            // check elements in map
            assert elem.containsKey("path")
            assert elem["path"] instanceof CharSequence
            if (elem.containsKey("mode")) {
              assert elem["mode"] instanceof CharSequence
              assert elem["mode"] in [ "symlink", "rellink", "link", "copy", "copyNoFollow", "move" ]
            }
            if (elem.containsKey("overwrite")) {
              assert elem["overwrite"] instanceof Boolean
            }
            if (elem.containsKey("pattern")) {
              assert elem["pattern"] instanceof CharSequence
            }
            if (elem.containsKey("saveAs")) {
              assert elem["saveAs"] instanceof CharSequence //: "saveAs as a Closure is currently not supported. Surround your closure with single quotes to get the desired effect. Example: '\{ foo \}'"
            }
            if (elem.containsKey("enabled")) {
              assert elem["enabled"] instanceof Boolean
            }
      
            // return final result
            elem
          }
          // store final directive
          drctv["publishDir"] = pblsh
        }
      
        /* DIRECTIVE queue
          accepted examples:
          - "long"
          - "short,long"
          - ["short", "long"]
        */
        if (drctv.containsKey("queue")) {
          if (drctv["queue"] instanceof List) {
            drctv["queue"] = drctv["queue"].join(",")
          }
          assert drctv["queue"] instanceof CharSequence
        }
      
        /* DIRECTIVE label
          accepted examples:
          - "big_mem"
          - "big_cpu"
          - ["big_mem", "big_cpu"]
        */
        if (drctv.containsKey("label")) {
          if (drctv["label"] instanceof CharSequence) {
            drctv["label"] = [ drctv["label"] ]
          }
          assert drctv["label"] instanceof List
          drctv["label"].forEach { label ->
            assert label instanceof CharSequence
            // assert label.matches("[a-zA-Z0-9]([a-zA-Z0-9_]*[a-zA-Z0-9])?")
            // ^ does not allow closures
          }
        }
      
        /* DIRECTIVE scratch
          accepted examples:
          - true
          - "/path/to/scratch"
          - '$MY_PATH_TO_SCRATCH'
          - "ram-disk"
        */
        if (drctv.containsKey("scratch")) {
          assert drctv["scratch"] == true || drctv["scratch"] instanceof CharSequence
        }
      
        /* DIRECTIVE storeDir
          accepted examples:
          - "/path/to/storeDir"
        */
        if (drctv.containsKey("storeDir")) {
          assert drctv["storeDir"] instanceof CharSequence
        }
      
        /* DIRECTIVE stageInMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageInMode")) {
          assert drctv["stageInMode"] instanceof CharSequence
          assert drctv["stageInMode"] in ["copy", "link", "symlink", "rellink"]
        }
      
        /* DIRECTIVE stageOutMode
          accepted examples:
          - "copy"
          - "link"
        */
        if (drctv.containsKey("stageOutMode")) {
          assert drctv["stageOutMode"] instanceof CharSequence
          assert drctv["stageOutMode"] in ["copy", "move", "rsync"]
        }
      
        /* DIRECTIVE tag
          accepted examples:
          - "foo"
          - '$id'
        */
        if (drctv.containsKey("tag")) {
          assert drctv["tag"] instanceof CharSequence
        }
      
        /* DIRECTIVE time
          accepted examples:
          - "1h"
          - "2days"
          - "1day 6hours 3minutes 30seconds"
        */
        if (drctv.containsKey("time")) {
          assert drctv["time"] instanceof CharSequence
          // todo: validation regex?
        }
      
        return drctv
      }
      
      // TODO: unit test processAuto
      def processAuto(Map auto) {
        // remove null values
        auto = auto.findAll{k, v -> v != null}
      
        expectedKeys = ["simplifyInput", "simplifyOutput", "transcript", "publish"]
      
        // check whether expected keys are all booleans (for now)
        for (key in expectedKeys) {
          assert auto.containsKey(key)
          assert auto[key] instanceof Boolean
        }
      
        return auto.subMap(expectedKeys)
      }
      
      def processProcessArgs(Map args) {
        // override defaults with args
        def processArgs = thisDefaultProcessArgs + args
      
        // check whether 'key' exists
        assert processArgs.containsKey("key")
      
        // if 'key' is a closure, apply it to the original key
        if (processArgs["key"] instanceof Closure) {
          processArgs["key"] = processArgs["key"](thisFunctionality.name)
        }
        assert processArgs["key"] instanceof CharSequence
        assert processArgs["key"] ==~ /^[a-zA-Z_][a-zA-Z0-9_]*$/
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("directives")
        assert processArgs["directives"] instanceof Map
        processArgs["directives"] = processDirectives(thisDefaultProcessArgs.directives + processArgs["directives"])
      
        // check whether directives exists and apply defaults
        assert processArgs.containsKey("auto")
        assert processArgs["auto"] instanceof Map
        processArgs["auto"] = processAuto(thisDefaultProcessArgs.auto + processArgs["auto"])
      
        // auto define publish, if so desired
        if (processArgs.auto.publish == true && (processArgs.directives.publishDir ?: [:]).isEmpty()) {
          assert params.containsKey("publishDir") : 
            "Error in module '${processArgs['key']}': if auto.publish is true, params.publishDir needs to be defined.\n" +
            "  Example: params.transcriptsDir = \"./output/\""
          
          // TODO: more asserts on publishDir?
          processArgs.directives.publishDir = [[ 
            path: params.publishDir, 
            saveAs: "{ it.startsWith('.') ? null : it }", // don't publish hidden files, by default
            mode: "copy"
          ]]
        }
      
        // auto define transcript, if so desired
        if (processArgs.auto.transcript == true) {
          assert params.containsKey("transcriptsDir") || params.containsKey("publishDir") : 
            "Error in module '${processArgs['key']}': if auto.transcript is true, either params.transcriptsDir or params.publishDir needs to be defined.\n" +
            "  Example: params.transcriptsDir = \"./transcripts/\""
          def transcriptsDir = params.containsKey("transcriptsDir") ? params.transcriptsDir : params.publishDir + "/_transcripts"
          def timestamp = Nextflow.getSession().getWorkflowMetadata().start.format('yyyy-MM-dd_HH-mm-ss')
          def transcriptsPublishDir = [ 
            path: "$transcriptsDir/$timestamp/\${task.process.replaceAll(':', '-')}/\${id}/", 
            saveAs: "{ it.startsWith('.') ? it.replaceAll('^.', '') : null }", 
            mode: "copy"
          ]
          def publishDirs = processArgs.directives.publishDir ?: []
          processArgs.directives.publishDir = publishDirs + transcriptsPublishDir
        }
      
        for (nam in [ "map", "mapId", "mapData", "mapPassthrough" ]) {
          if (processArgs.containsKey(nam) && processArgs[nam]) {
            assert processArgs[nam] instanceof Closure : "Expected process argument '$nam' to be null or a Closure. Found: class ${processArgs[nam].getClass()}"
          }
        }
      
        // return output
        return processArgs
      }
      
      def processFactory(Map processArgs) {
        def tripQuo = "\"\"\""
      
        // autodetect process key
        def wfKey = processArgs["key"]
        def procKeyPrefix = "${wfKey}_process"
        def meta = ScriptMeta.current()
        def existing = meta.getProcessNames().findAll{it.startsWith(procKeyPrefix)}
        def numbers = existing.collect{it.replace(procKeyPrefix, "0").toInteger()}
        def newNumber = (numbers + [-1]).max() + 1
      
        def procKey = newNumber == 0 ? procKeyPrefix : "$procKeyPrefix$newNumber"
      
        if (newNumber > 0) {
          log.warn "Key for module '${wfKey}' is duplicated.\n",
            "If you run a component multiple times in the same workflow,\n" +
            "it's recommended you set a unique key for every call,\n" +
            "for example: ${wfKey}.run(key: \"foo\")."
        }
      
        // subset directives and convert to list of tuples
        def drctv = processArgs.directives
      
        // TODO: unit test the two commands below
        // convert publish array into tags
        def valueToStr = { val ->
          // ignore closures
          if (val instanceof CharSequence) {
            if (!val.matches('^[{].*[}]$')) {
              '"' + val + '"'
            } else {
              val
            }
          } else if (val instanceof List) {
            "[" + val.collect{valueToStr(it)}.join(", ") + "]"
          } else if (val instanceof Map) {
            "[" + val.collect{k, v -> k + ": " + valueToStr(v)}.join(", ") + "]"
          } else {
            val.inspect()
          }
        }
        // multiple entries allowed: label, publishdir
        def drctvStrs = drctv.collect { key, value ->
          if (key in ["label", "publishDir"]) {
            value.collect{ val ->
              if (val instanceof Map) {
                "\n$key " + val.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
              } else {
                "\n$key " + valueToStr(val)
              }
            }.join()
          } else if (value instanceof Map) {
            "\n$key " + value.collect{ k, v -> k + ": " + valueToStr(v) }.join(", ")
          } else {
            "\n$key " + valueToStr(value)
          }
        }.join()
      
        def inputPaths = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "input" }
          .collect { ', path(viash_par_' + it.name + ')' }
          .join()
      
        def outputPaths = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par ->
            // insert dummy into every output (see nextflow-io/nextflow#2678)
            if (!par.multiple) {
              ', path{[".exitcode", args.' + par.name + ']}'
            } else {
              ', path{[".exitcode"] + args.' + par.name + '}'
            }
          }
          .join()
      
        // TODO: move this functionality somewhere else?
        if (processArgs.auto.transcript) {
          outputPaths = outputPaths + ', path{[".exitcode", ".command*"]}'
        } else {
          outputPaths = outputPaths + ', path{[".exitcode"]}'
        }
      
        // construct inputFileExports
        def inputFileExports = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction.toLowerCase() == "input" }
          .collect { par ->
            viash_par_contents = !par.required && !par.multiple ? "viash_par_${par.name}[0]" : "viash_par_${par.name}.join(\":\")"
            "\n\${viash_par_${par.name}.empty ? \"\" : \"export VIASH_PAR_${par.name.toUpperCase()}=\\\"\" + ${viash_par_contents} + \"\\\"\"}"
          }
        
        def tmpDir = "/tmp" // check if component is docker based
      
        // construct stub
        def stub = thisFunctionality.arguments
          .findAll { it.type == "file" && it.direction == "output" }
          .collect { par -> 
            'touch "${viash_par_' + par.name + '.join(\'" "\')}"'
          }
          .join("\n")
      
        // escape script
        def escapedScript = thisScript.replace('\\', '\\\\').replace('$', '\\$').replace('"""', '\\"\\"\\"')
      
        // generate process string
        def procStr = 
        """nextflow.enable.dsl=2
        |
        |process $procKey {$drctvStrs
        |input:
        |  tuple val(id)$inputPaths, val(args), val(passthrough), path(resourcesDir)
        |output:
        |  tuple val("\$id"), val(passthrough)$outputPaths, optional: true
        |stub:
        |$tripQuo
        |$stub
        |$tripQuo
        |script:
        |def escapeText = { s -> s.toString().replaceAll('([`"])', '\\\\\\\\\$1') }
        |def parInject = args
        |  .findAll{key, value -> value != null}
        |  .collect{key, value -> "export VIASH_PAR_\${key.toUpperCase()}=\\\"\${escapeText(value)}\\\""}
        |  .join("\\n")
        |$tripQuo
        |# meta exports
        |export VIASH_META_RESOURCES_DIR="\${resourcesDir.toRealPath().toAbsolutePath()}"
        |export VIASH_META_TEMP_DIR="${tmpDir}"
        |export VIASH_META_FUNCTIONALITY_NAME="${thisFunctionality.name}"
        |
        |# meta synonyms
        |export VIASH_RESOURCES_DIR="\\\$VIASH_META_RESOURCES_DIR"
        |export VIASH_TEMP="\\\$VIASH_META_TEMP_DIR"
        |export TEMP_DIR="\\\$VIASH_META_TEMP_DIR"
        |
        |# argument exports${inputFileExports.join()}
        |\$parInject
        |
        |# process script
        |${escapedScript}
        |$tripQuo
        |}
        |""".stripMargin()
      
        // TODO: print on debug
        // if (processArgs.debug == true) {
        //   println("######################\n$procStr\n######################")
        // }
      
        // create runtime process
        def ownerParams = new ScriptBinding.ParamsMap()
        def binding = new ScriptBinding().setParams(ownerParams)
        def module = new IncludeDef.Module(name: procKey)
        def moduleScript = new ScriptParser(session)
          .setModule(true)
          .setBinding(binding)
          .runScript(procStr)
          .getScript()
      
        // register module in meta
        meta.addModule(moduleScript, module.name, module.alias)
      
        // retrieve and return process from meta
        return meta.getProcess(procKey)
      }
      
      def debug(processArgs, debugKey) {
        if (processArgs.debug) {
          view { "process '${processArgs.key}' $debugKey tuple: $it"  }
        } else {
          map { it }
        }
      }
      
      // wfKeyCounter = -1
      
      def workflowFactory(Map args) {
        def processArgs = processProcessArgs(args)
        def key = processArgs["key"]
        def meta = ScriptMeta.current()
      
        // def workflowKey = wfKeyCounter == -1 ? key : "$key$wfKeyCounter"
        // wfKeyCounter++
        def workflowKey = key
      
        // write process to temporary nf file and parse it in memory
        def processObj = processFactory(processArgs)
        
        workflow workflowInstance {
          take:
          input_
      
          main:
          output_ = input_
            | debug(processArgs, "input")
            | map { tuple ->
              if (processArgs.map) {
                tuple = processArgs.map(tuple)
              }
              if (processArgs.mapId) {
                tuple[0] = processArgs.mapId(tuple[0])
              }
              if (processArgs.mapData) {
                tuple[1] = processArgs.mapData(tuple[1])
              }
              if (processArgs.mapPassthrough) {
                tuple = tuple.take(2) + processArgs.mapPassthrough(tuple.drop(2))
              }
      
              // check tuple
              assert tuple instanceof List : 
                "Error in module '${key}': element in channel should be a tuple [id, data, ...otherargs...]\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: List. Found: tuple.getClass() is ${tuple.getClass()}"
              assert tuple.size() >= 2 : 
                "Error in module '${key}': expected length of tuple in input channel to be two or greater.\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: tuple.size() == ${tuple.size()}"
              
              // check id field
              assert tuple[0] instanceof CharSequence : 
                "Error in module '${key}': first element of tuple in channel should be a String\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Found: ${tuple[0]}"
              
              // match file to input file
              if (processArgs.auto.simplifyInput && (tuple[1] instanceof Path || tuple[1] instanceof List)) {
                def inputFiles = thisFunctionality.arguments
                  .findAll { it.type == "file" && it.direction == "input" }
                
                assert inputFiles.size() == 1 : 
                    "Error in module '${key}' id '${tuple[0]}'.\n" +
                    "  Anonymous file inputs are only allowed when the process has exactly one file input.\n" +
                    "  Expected: inputFiles.size() == 1. Found: inputFiles.size() is ${inputFiles.size()}"
      
                tuple[1] = [[ inputFiles[0].name, tuple[1] ]].collectEntries()
              }
      
              // check data field
              assert tuple[1] instanceof Map : 
                "Error in module '${key}' id '${tuple[0]}': second element of tuple in channel should be a Map\n" +
                "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
                "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
              // rename keys of data field in tuple
              if (processArgs.renameKeys) {
                assert processArgs.renameKeys instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class: Map. Found: renameKeys.getClass() is ${processArgs.renameKeys.getClass()}"
                assert tuple[1] instanceof Map : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"
      
                // TODO: allow renameKeys to be a function?
                processArgs.renameKeys.each { newKey, oldKey ->
                  assert newKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of newKey: String. Found: newKey.getClass() is ${newKey.getClass()}"
                  assert oldKey instanceof CharSequence : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Example: renameKeys: ['new_key': 'old_key'].\n" +
                    "  Expected class of oldKey: String. Found: oldKey.getClass() is ${oldKey.getClass()}"
                  assert tuple[1].containsKey(oldKey) : 
                    "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
                    "  Key '$oldKey' is missing in the data map. tuple[1].keySet() is '${tuple[1].keySet()}'"
                  tuple[1].put(newKey, tuple[1][oldKey])
                }
                tuple[1].keySet().removeAll(processArgs.renameKeys.collect{ newKey, oldKey -> oldKey })
              }
              tuple
            }
            | debug(processArgs, "processed")
            | map { tuple ->
              def id = tuple[0]
              def data = tuple[1]
              def passthrough = tuple.drop(2)
      
              // fetch default params from functionality
              def defaultArgs = thisFunctionality.arguments
                .findAll { it.containsKey("default") }
                .collectEntries { [ it.name, it.default ] }
      
              // fetch overrides in params
              def paramArgs = thisFunctionality.arguments
                .findAll { par ->
                  def argKey = key + "__" + par.name
                  params.containsKey(argKey) && params[argKey] != "viash_no_value"
                }
                .collectEntries { [ it.name, params[key + "__" + it.name] ] }
              
              // fetch overrides in data
              def dataArgs = thisFunctionality.arguments
                .findAll { data.containsKey(it.name) }
                .collectEntries { [ it.name, data[it.name] ] }
              
              // combine params
              def combinedArgs = defaultArgs + paramArgs + processArgs.args + dataArgs
      
              // remove arguments with explicit null values
              combinedArgs.removeAll{it == null}
      
              // check whether required arguments exist
              thisFunctionality.arguments
                .forEach { par ->
                  if (par.required) {
                    assert combinedArgs.containsKey(par.name): "Argument ${par.name} is required but does not have a value"
                  }
                }
      
              // TODO: check whether parameters have the right type
      
              // process input files separately
              def inputPaths = thisFunctionality.arguments
                .findAll { it.type == "file" && it.direction == "input" }
                .collect { par ->
                  def val = combinedArgs.containsKey(par.name) ? combinedArgs[par.name] : []
                  def inputFiles = []
                  if (val == null) {
                    inputFiles = []
                  } else if (val instanceof List) {
                    inputFiles = val
                  } else if (val instanceof Path) {
                    inputFiles = [ val ]
                  } else {
                    inputFiles = []
                  }
                  // throw error when an input file doesn't exist
                  inputFiles.each{ file -> 
                    assert file.exists() :
                      "Error in module '${key}' id '${id}' argument '${par.name}'.\n" +
                      "  Required input file does not exist.\n" +
                      "  Path: '$file'.\n" +
                      "  Expected input file to exist"
                  }
                  inputFiles 
                } 
      
              // remove input files
              def argsExclInputFiles = thisFunctionality.arguments
                .findAll { it.type != "file" || it.direction != "input" }
                .collectEntries { par ->
                  def parName = par.name
                  def val = combinedArgs[parName]
                  if (par.multiple && val instanceof Collection) {
                    val = val.join(par.multiple_sep)
                  }
                  if (par.direction == "output" && par.type == "file") {
                    val = val.replaceAll('\\$id', id).replaceAll('\\$key', key)
                  }
                  [parName, val]
                }
      
              [ id ] + inputPaths + [ argsExclInputFiles, passthrough, resourcesDir ]
            }
            | processObj
            | map { output ->
              def outputFiles = thisFunctionality.arguments
                .findAll { it.type == "file" && it.direction == "output" }
                .indexed()
                .collectEntries{ index, par ->
                  out = output[index + 2]
                  // strip dummy '.exitcode' file from output (see nextflow-io/nextflow#2678)
                  if (!out instanceof List || out.size() <= 1) {
                    if (par.multiple) {
                      out = []
                    } else {
                      assert !par.required :
                          "Error in module '${key}' id '${output[0]}' argument '${par.name}'.\n" +
                          "  Required output file is missing"
                      out = null
                    }
                  } else if (out.size() == 2 && !par.multiple) {
                    out = out[1]
                  } else {
                    out = out.drop(1)
                  }
                  [ par.name, out ]
                }
              
              // drop null outputs
              outputFiles.removeAll{it.value == null}
      
              if (processArgs.auto.simplifyOutput && outputFiles.size() == 1) {
                outputFiles = outputFiles.values()[0]
              }
      
              def out = [ output[0], outputFiles ]
      
              // passthrough additional items
              if (output[1]) {
                out.addAll(output[1])
              }
      
              out
            }
            | debug(processArgs, "output")
      
          emit:
          output_
        }
      
        def wf = workflowInstance.cloneWithName(workflowKey)
      
        // add factory function
        wf.metaClass.run = { runArgs ->
          workflowFactory(runArgs)
        }
      
        return wf
      }
      
      // initialise default workflow
      myWfInstance = workflowFactory([:])
      
      // add workflow to environment
      ScriptMeta.current().addDefinition(myWfInstance)
      
      // anonymous workflow for running this module as a standalone
      workflow {
        if (params.containsKey("help") && params["help"]) {
          exit 0, thisHelpMessage
        }
        if (!params.containsKey("id")) {
          params.id = "run"
        }
        if (!params.containsKey("publishDir")) {
          params.publishDir = "./"
        }
      
        // fetch parameters
        def args = thisFunctionality.arguments
          .findAll { par -> params.containsKey(par.name) }
          .collectEntries { par ->
            if (par.type == "file" && par.direction == "input") {
              [ par.name, file(params[par.name]) ]
            } else {
              [ par.name, params[par.name] ]
            }
          }
                
        Channel.value([ params.id, args ])
          | view { "input: $it" }
          | myWfInstance.run(
            auto: [ publish: true ]
          )
          | view { "output: $it" }
      }

    dest: "main.nf"
  - type: "file"
    text: |
      manifest {
        name = 'bd_rhapsody_wta'
        mainScript = 'main.nf'
        nextflowVersion = '!>=20.12.1-edge'
        version = 'main_build'
        description = 'A wrapper for the BD Rhapsody Analysis CWL v1.9.1 pipeline.\n\nThe CWL pipeline file is obtained by cloning \'https://bitbucket.org/CRSwDev/cwl/src/master/\' and removing all objects with class \'DockerRequirement\' from the YML.\n\nThe reference_genome and transcriptome_annotation files can be downloaded from these locations:\n  - Human: http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCh38-PhiX-gencodev29/\n  - Mouse: http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCm38-PhiX-gencodevM19/\n'
        author = 'Robrecht Cannoodt <rcannood@gmail.com> (maintainer) {github: rcannood, orcid: 0000-0003-3641-729X}'
      }
      
      // detect tempdir
      tempDir = java.nio.file.Paths.get(
        System.getenv('NXF_TEMP') ?:
          System.getenv('VIASH_TEMP') ?: 
          System.getenv('TEMPDIR') ?: 
          System.getenv('TMPDIR') ?: 
          '/tmp'
      ).toAbsolutePath()
      
      profiles {
        docker {
          docker.enabled         = true
          docker.userEmulation   = true
          docker.temp            = tempDir
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        singularity {
          singularity.enabled    = true
          singularity.autoMounts = true
          docker.enabled         = false
          podman.enabled         = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        podman {
          podman.enabled         = true
          podman.temp            = tempDir
          docker.enabled         = false
          singularity.enabled    = false
          shifter.enabled        = false
          charliecloud.enabled   = false
        }
        shifter {
          shifter.enabled        = true
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          charliecloud.enabled   = false
        }
        charliecloud {
          charliecloud.enabled   = true
          charliecloud.temp      = tempDir
          docker.enabled         = false
          singularity.enabled    = false
          podman.enabled         = false
          shifter.enabled        = false
        }
      }

    dest: "nextflow.config"
  - type: "file"
    path: "rhapsody_wta_1.9.1_nodocker.cwl"
  description: "A wrapper for the BD Rhapsody Analysis CWL v1.9.1 pipeline.\n\nThe\
    \ CWL pipeline file is obtained by cloning 'https://bitbucket.org/CRSwDev/cwl/src/master/'\
    \ and removing all objects with class 'DockerRequirement' from the YML.\n\nThe\
    \ reference_genome and transcriptome_annotation files can be downloaded from these\
    \ locations:\n  - Human: http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCh38-PhiX-gencodev29/\n\
    \  - Mouse: http://bd-rhapsody-public.s3-website-us-east-1.amazonaws.com/Rhapsody-WTA/GRCm38-PhiX-gencodevM19/\n"
  tests:
  - type: "bash_script"
    path: "run_test.sh"
    is_executable: true
  - type: "file"
    path: "../../../resources_test/bd_rhapsody_wta_test"
  info: {}
  dummy_arguments: []
  set_wd_to_resources_dir: false
  enabled: true
platform:
  type: "nextflow"
  id: "nextflow"
  variant: "vdsl3"
  directives:
    accelerator: {}
    conda: []
    containerOptions: []
    label:
    - "highmem"
    - "highcpu"
    module: []
    pod: []
    publishDir: []
    queue: []
  auto:
    simplifyInput: true
    simplifyOutput: true
    transcript: false
    publish: false
  debug: false
  container: "docker"
platforms: []
info:
  config: "src/mapping/bd_rhapsody_wta/config.vsh.yaml"
  platform: "nextflow"
  output: "target/nextflow/mapping/bd_rhapsody_wta"
  viash_version: "0.5.13"
  git_commit: "8f82d30ddc08e19f7321c18b80c80cfd163efc3e"
  git_remote: "https://github.com/openpipelines-bio/openpipeline"
  git_tag: "fatal: No names found, cannot describe anything."
